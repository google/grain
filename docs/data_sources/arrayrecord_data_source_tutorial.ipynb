{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQk1knFmoaw_"
      },
      "source": [
        "# ArrayRecordDataSource\n",
        "\n",
        "This tutorial provides an example of how to retrieve records from ArrayRecord files using `grain.sources.ArrayRecordDataSource`, also covers how to process and transform the data with Grain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW0hSbMYtn0s"
      },
      "source": [
        "## Read records from ArrayRecord files\n",
        "This section reads records from ArrayRecord files, also defines an example transform function to parse and tokenize the record data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5hl65buTyyz"
      },
      "source": [
        "### Define File Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp7aw9RPxDTf"
      },
      "outputs": [],
      "source": [
        "import grain\n",
        "import numpy as np\n",
        "from tensorflow_datasets.core.constants import ARRAY_RECORD_DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFw9NZw3w0Tb"
      },
      "outputs": [],
      "source": [
        "# The grain.sources.ArrayRecordDataSource supports sharded file path.\n",
        "example_file_paths = (\n",
        "    ARRAY_RECORD_DATA_DIR + '/aeslc/1.0.0/aeslc-train.array_record@1'\n",
        ")\n",
        "print(example_file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMJWuoz4hkm_"
      },
      "outputs": [],
      "source": [
        "# @title Load Data Source\n",
        "example_array_record_data_source = grain.sources.ArrayRecordDataSource(example_file_paths)\n",
        "print(f\"Number of records: {len(example_array_record_data_source)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4NmWZ7nVYML"
      },
      "source": [
        "### Define Transformation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4de_xw1AKC4"
      },
      "outputs": [],
      "source": [
        "# Load a pre trained tokenizer\n",
        "from tokenizers import Tokenizer\n",
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8PVSCTEeLA9"
      },
      "outputs": [],
      "source": [
        "class ParseAndTokenizeText(grain.transforms.Map):\n",
        "  \"\"\"Parses a serialized TF.Example containing a 'text' feature and tokenizes it.\n",
        "\n",
        "  The 'text' feature is expected to be a list of bytes. This function decodes\n",
        "  the bytes to UTF-8 string, tokenizes it using the provided tokenizer, flattens\n",
        "  the resulting list of token IDs, and returns the first 10 tokens.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, tokenizer):\n",
        "    self._tokenizer = tokenizer\n",
        "\n",
        "  def map(self, proto_bytes: bytes) -\u003e [str]:\n",
        "    # parse individual data record\n",
        "    parsed_element = grain.fast_proto.parse_tf_example_experimental(\n",
        "        proto_bytes, strip_trailing_null_characters=True\n",
        "    )\n",
        "    tokens = [\n",
        "        self._tokenizer.encode(item.decode('utf-8')).tokens\n",
        "        for item in parsed_element['email_body']\n",
        "    ]\n",
        "    tokens = np.array(tokens).flatten()\n",
        "    # only pick the first 10 token IDs from the tokenized text for testing\n",
        "    return tokens[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPIy05gGUBzI"
      },
      "outputs": [],
      "source": [
        "# Example using Grain's MapDataset with ArrayRecord file source.\n",
        "example_datasets = (\n",
        "    grain.MapDataset.source(example_array_record_data_source)\n",
        "    .shuffle(seed=42)\n",
        "    .map(ParseAndTokenizeText(tokenizer))\n",
        "    .batch(batch_size=10)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqJSeQ9hdAmF"
      },
      "outputs": [],
      "source": [
        "# Output a record at a random index\n",
        "print(example_datasets[100])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1JbSnqHDYHFt_nLm24U_xmtL8czPgUaU6",
          "timestamp": 1744760988378
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
