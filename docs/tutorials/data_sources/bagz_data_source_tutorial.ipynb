{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7696e57f",
   "metadata": {},
   "source": [
    "# Using Bagz Files\n",
    "\n",
    "This tutorial gives an overview of integrating [Bagz](https://github.com/google-deepmind/bagz/) file format into Grain pipeline. Bagz, an alternative to ArrayRecord, is a novel file format which supports per-record compression and fast index-based lookup. It can also integrate with Apache Beam, a feature that we're also going to present in this tutorial.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we need to make sure we have all required packages. We pin JAX's version as the latest Apache Beam doesn't support NumPy 2.0 yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install grain bagz apache-beam jax==0.4.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa58f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grain\n",
    "import bagz\n",
    "from bagz.beam import bagzio\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import random\n",
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fe0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.__version__[0] == \"1\", \"Apache Beam requires NumPy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f3b31",
   "metadata": {},
   "source": [
    "## Creating and reading Bagz files\n",
    "\n",
    "As Bagz format is record-based we can use a simple loop and `bagz.Writer` context manager to write our contents to the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2dc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "records = list(f\"Record: {random.randint(100, 1000)}\" for _ in range(40))\n",
    "\n",
    "file = pathlib.Path(\"data.bagz\")\n",
    "\n",
    "with bagz.Writer(file) as writer:\n",
    "    for rec in records:\n",
    "        writer.write(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a84846",
   "metadata": {},
   "source": [
    "Bagz supports random access, therefore we can lookup items by index, check length of the file, and slice it arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1811d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "b'Record: 792'\n",
      "[b'Record: 350', b'Record: 328', b'Record: 242', b'Record: 854', b'Record: 204', b'Record: 792', b'Record: 858', b'Record: 658', b'Record: 189', b'Record: 704']\n"
     ]
    }
   ],
   "source": [
    "reader = bagz.Reader(file)\n",
    "\n",
    "print(len(reader))\n",
    "\n",
    "print(reader[10])\n",
    "\n",
    "print(list(reader[5:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c50d08",
   "metadata": {},
   "source": [
    "## Grain pipeline with Bagz files\n",
    "\n",
    "With random access in mind, we can now consume Bagz files in a Grain pipeline with `grain.MapDataset` class. Then applying any transformation is the same as with other sources, such as ArrayRecord files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94496e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    grain.MapDataset.source(reader)\n",
    "    .shuffle(seed=42)\n",
    "    .map(lambda x: x.decode())  # move from bytes to strings\n",
    "    .filter(lambda x: x[-1] != \"6\")  # let's filter out some files\n",
    "    .map(lambda x: x.upper())  # and capitalize them\n",
    "    .to_iter_dataset()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0002bb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out: 2 records.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RECORD: 704',\n",
       " 'RECORD: 674',\n",
       " 'RECORD: 877',\n",
       " 'RECORD: 189',\n",
       " 'RECORD: 325',\n",
       " 'RECORD: 323',\n",
       " 'RECORD: 303',\n",
       " 'RECORD: 125',\n",
       " 'RECORD: 381',\n",
       " 'RECORD: 990',\n",
       " 'RECORD: 127',\n",
       " 'RECORD: 859',\n",
       " 'RECORD: 858',\n",
       " 'RECORD: 204',\n",
       " 'RECORD: 854',\n",
       " 'RECORD: 350',\n",
       " 'RECORD: 132',\n",
       " 'RECORD: 338',\n",
       " 'RECORD: 928',\n",
       " 'RECORD: 532',\n",
       " 'RECORD: 328',\n",
       " 'RECORD: 818',\n",
       " 'RECORD: 833',\n",
       " 'RECORD: 658',\n",
       " 'RECORD: 195',\n",
       " 'RECORD: 214',\n",
       " 'RECORD: 529',\n",
       " 'RECORD: 765',\n",
       " 'RECORD: 617',\n",
       " 'RECORD: 384',\n",
       " 'RECORD: 658',\n",
       " 'RECORD: 559',\n",
       " 'RECORD: 703',\n",
       " 'RECORD: 925',\n",
       " 'RECORD: 130',\n",
       " 'RECORD: 792',\n",
       " 'RECORD: 242',\n",
       " 'RECORD: 754']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Filtered out: {len(reader) - len(list(dataset))} records.\")\n",
    "\n",
    "list(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6ceca",
   "metadata": {},
   "source": [
    "## Apache Beam\n",
    "\n",
    "Likewise ArrayRecord, Bagz package can also integrate with the Apache Beam library to build ETL pipelines. In the example below we construct a pipeline which consumes some in-memory list, performs simple transformations, and loads outputs to a Bagz file with a `bagzio` module. `@0` in the filename indicates that we don't want sharding for this pipeline. To learn more about sharding in Bagz, please see [Bagz docs](https://github.com/google-deepmind/bagz/tree/main?tab=readme-ov-file#sharding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e6221fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "  data = [\"record1\", \"record2\", \"record3\"]\n",
    "  _ = (\n",
    "      pipeline\n",
    "      | 'CreateData' >> beam.Create(data)\n",
    "      | 'Capitalize' >> beam.Map(lambda x: x.upper())\n",
    "      | 'Encode' >> beam.Map(lambda x: x.encode())\n",
    "      | 'WriteData' >> bagzio.WriteToBagz('beam_data@0.bagz')\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35aefdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'RECORD1', b'RECORD2', b'RECORD3']\n"
     ]
    }
   ],
   "source": [
    "file = pathlib.Path(\"beam_data.bagz\")\n",
    "reader = bagz.Reader(file)\n",
    "print(list(reader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c3411",
   "metadata": {},
   "source": [
    "In this tutorial we've learned about Bagz format."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3",
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "grain-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
