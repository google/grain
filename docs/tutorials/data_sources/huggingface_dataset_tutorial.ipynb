{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6849dc",
   "metadata": {},
   "source": [
    "# Loading and transforming HuggingFace datasets\n",
    "\n",
    "HuggingFace (HF) platform provides a wide variety of ML models, datasets, and transformers for the worldwide community.\n",
    "An easy access to these assets is guaranteed thanks to Python packages such as [datasets](https://pypi.org/project/datasets/) or [transformers](https://pypi.org/project/transformers/), available on PyPI.\n",
    "\n",
    "In this tutorial you will learn how to utilize HF datasets and tools with Grain: How to load HF datasets and how to use HF transformers in your Grain pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054361b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To run the notebook you need to have a few packages installed in your environment: `grain`, `numpy`, and Two HF packages: `datasets` and `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b72304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @test {\"output\": \"ignore\"}\n",
    "!pip install grain\n",
    "# @test {\"output\": \"ignore\"}\n",
    "!pip install numpy datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59abb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "from pprint import pprint\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import grain\n",
    "import numpy as np\n",
    "\n",
    "# HF imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e4aab8",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "Let's first import an HF dataset. For the sake of simplicity let's proceed with [lhoestq/demo1](https://huggingface.co/datasets/lhoestq/demo1) - a minimal dataset comprised of five rows and six columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b2dbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'package_name', 'review', 'date', 'star', 'version_id'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'package_name', 'review', 'date', 'star', 'version_id'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = load_dataset(\"lhoestq/demo1\")\n",
    "hf_train, hf_test = hf_dataset[\"train\"], hf_dataset[\"test\"]\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d93d5f",
   "metadata": {},
   "source": [
    "Each sample is a Python dictionary with string or integer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e39b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7bd227d9-afc9-11e6-aba1-c4b301cdf627',\n",
       " 'package_name': 'com.mantz_it.rfanalyzer',\n",
       " 'review': \"Great app! The new version now works on my Bravia Android TV which is great as it's right by my rooftop aerial cable. The scan feature would be useful...any ETA on when this will be available? Also the option to import a list of bookmarks e.g. from a simple properties file would be useful.\",\n",
       " 'date': 'October 12 2016',\n",
       " 'star': 4,\n",
       " 'version_id': 1487}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10aa33",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Let's assume that for our preprocessing pipeline we want the string `date` field to become a timestamp and the whole sample - a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660fb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(sample: dict) -> dict:\n",
    "    sample[\"date\"] = parse(sample[\"date\"]).timestamp()\n",
    "    return sample\n",
    "\n",
    "def process_sample_to_np(sample: dict) -> np.ndarray:\n",
    "    return np.array([*sample.values()], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed32ae1",
   "metadata": {},
   "source": [
    "Building a pipeline is as simple as chaining `map` calls. HF dataset supports random access so we can pass it directly to a `source` method. The resulting object is of type `grain.MapDataset` with random access support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c041728",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    grain.MapDataset.source(hf_train)\n",
    "    .shuffle(seed=10)  # shuffles globally\n",
    "    .map(process_date)  # maps each element\n",
    "    .map(process_sample_to_np)  # maps each element\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c502f8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['7bd22aba-afc9-11e6-8293-c4b301cdf627', 'com.mantz_it.rfanalyzer',\n",
       "        'Works well with my Hackrf Hopefully new updates will arrive for extra functions',\n",
       "        1469145600.0, 5, 1487], dtype=object),\n",
       " array(['7bd227d9-afc9-11e6-aba1-c4b301cdf627', 'com.mantz_it.rfanalyzer',\n",
       "        \"Great app! The new version now works on my Bravia Android TV which is great as it's right by my rooftop aerial cable. The scan feature would be useful...any ETA on when this will be available? Also the option to import a list of bookmarks e.g. from a simple properties file would be useful.\",\n",
       "        1476230400.0, 4, 1487], dtype=object),\n",
       " array(['7bd22905-afc9-11e6-a5dc-c4b301cdf627', 'com.mantz_it.rfanalyzer',\n",
       "        \"Great It's not fully optimised and has some issues with crashing but still a nice app  especially considering the price and it's open source.\",\n",
       "        1471910400.0, 4, 1487], dtype=object),\n",
       " array(['7bd22a26-afc9-11e6-9309-c4b301cdf627', 'com.mantz_it.rfanalyzer',\n",
       "        'The bandwidth seemed to be limited to maximum 2 MHz or so. I tried to increase the bandwidth but not possible. I purchased this is because one of the pictures in the advertisement showed the 2.4GHz band with around 10MHz or more bandwidth. Is it not possible to increase the bandwidth? If not  it is just the same performance as other free APPs.',\n",
       "        1469404800.0, 3, 1487], dtype=object),\n",
       " array(['7bd2299c-afc9-11e6-85d6-c4b301cdf627', 'com.mantz_it.rfanalyzer',\n",
       "        \"Works on a Nexus 6p I'm still messing around with my hackrf but it works with my Nexus 6p  Trond usb-c to usb host adapter. Thanks!\",\n",
       "        1470268800.0, 5, 1487], dtype=object)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c11d70",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Next we would like to tokenize the `review` field. LLM models operate on integers (encoded words) rather than raw strings. `AutoTokenizer` generic class ships `from_pretrained` method - accessor to models and tokenizers hosted on HF services.\n",
    "\n",
    "Let's use `bert-base-uncased`, a case-insensitive BERT-based transformers model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca74baf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c6cd6",
   "metadata": {},
   "source": [
    "Transforming a single review string yields a dictionary with three keys. We're only interested in `input_ids` since that is the encoded review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf90949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Great app! The new version now works on my Bravia Android TV which is great '\n",
      " \"as it's right by my rooftop aerial cable. The scan feature would be \"\n",
      " 'useful...any ETA on when this will be available? Also the option to import a '\n",
      " 'list of bookmarks e.g. from a simple properties file would be useful.')\n",
      "\n",
      " dict_keys(['input_ids', 'token_type_ids', 'attention_mask']) \n",
      "\n",
      "[101,\n",
      " 2307,\n",
      " 10439,\n",
      " 999,\n",
      " 1996,\n",
      " 2047,\n",
      " 2544,\n",
      " 2085,\n",
      " 2573,\n",
      " 2006,\n",
      " 2026,\n",
      " 11655,\n",
      " 9035,\n",
      " 11924,\n",
      " 2694,\n",
      " 2029,\n",
      " 2003,\n",
      " 2307,\n",
      " 2004,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 2157,\n",
      " 2011,\n",
      " 2026,\n",
      " 23308,\n",
      " 9682,\n",
      " 5830,\n",
      " 1012,\n",
      " 1996,\n",
      " 13594,\n",
      " 3444,\n",
      " 2052,\n",
      " 2022,\n",
      " 6179,\n",
      " 1012,\n",
      " 1012,\n",
      " 1012,\n",
      " 2151,\n",
      " 27859,\n",
      " 2006,\n",
      " 2043,\n",
      " 2023,\n",
      " 2097,\n",
      " 2022,\n",
      " 2800,\n",
      " 1029,\n",
      " 2036,\n",
      " 1996,\n",
      " 5724,\n",
      " 2000,\n",
      " 12324,\n",
      " 1037,\n",
      " 2862,\n",
      " 1997,\n",
      " 2338,\n",
      " 27373,\n",
      " 1041,\n",
      " 1012,\n",
      " 1043,\n",
      " 1012,\n",
      " 2013,\n",
      " 1037,\n",
      " 3722,\n",
      " 5144,\n",
      " 5371,\n",
      " 2052,\n",
      " 2022,\n",
      " 6179,\n",
      " 1012,\n",
      " 102]\n"
     ]
    }
   ],
   "source": [
    "review = hf_train[0][\"review\"]\n",
    "pprint(review)\n",
    "print(\"\\n\", tokenizer(review).keys(), \"\\n\")\n",
    "pprint(tokenizer(review)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1544b",
   "metadata": {},
   "source": [
    "Plugging the selected transformer is as easy as before. We implement the `process_transformer` function and pass it to the `map` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01e1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transformer(sample: dict) -> dict:\n",
    "    sample[\"review\"] = np.array(tokenizer(sample[\"review\"])[\"input_ids\"])\n",
    "    return sample\n",
    "\n",
    "dataset = (\n",
    "    grain.MapDataset.source(hf_train)\n",
    "    .shuffle(seed=10)\n",
    "    .map(process_date)\n",
    "    .map(process_transformer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ccd22",
   "metadata": {},
   "source": [
    "Now samples are less human- but more machine-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68c05c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7bd227d9-afc9-11e6-aba1-c4b301cdf627',\n",
       " 'package_name': 'com.mantz_it.rfanalyzer',\n",
       " 'review': array([  101,  2307, 10439,   999,  1996,  2047,  2544,  2085,  2573,\n",
       "         2006,  2026, 11655,  9035, 11924,  2694,  2029,  2003,  2307,\n",
       "         2004,  2009,  1005,  1055,  2157,  2011,  2026, 23308,  9682,\n",
       "         5830,  1012,  1996, 13594,  3444,  2052,  2022,  6179,  1012,\n",
       "         1012,  1012,  2151, 27859,  2006,  2043,  2023,  2097,  2022,\n",
       "         2800,  1029,  2036,  1996,  5724,  2000, 12324,  1037,  2862,\n",
       "         1997,  2338, 27373,  1041,  1012,  1043,  1012,  2013,  1037,\n",
       "         3722,  5144,  5371,  2052,  2022,  6179,  1012,   102]),\n",
       " 'date': 1476230400.0,\n",
       " 'star': 4,\n",
       " 'version_id': 1487}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cc2a1",
   "metadata": {},
   "source": [
    "## Complete Pipeline\n",
    "\n",
    "Time to build our final pipeline! The pipeline doesn't need to be restricted to `shuffle` and `map`. Grain has a rich API and hands us multiple functionalities such as: `filter`, `random_map`, `repeat`. Check out [Grain API](../../grain.rst) page to learn more.\n",
    "\n",
    "On top of the transformer we want to discard reviews that are rated three stars or less. It's crucial to mention that filtering changes the number of samples in the following steps so random access is no longer available. To perform `batching` as the final step we plug `.to_iter_dataset()` converting `MapDataset` to `IterDataset` - a dataset that gives us an iterator-like interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4043597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    grain.MapDataset.source(hf_train)\n",
    "    .shuffle(seed=10)\n",
    "    .map(process_date)\n",
    "    .map(process_transformer)\n",
    "    .filter(lambda x: x[\"star\"] > 3)  # filters samples\n",
    "    .map(process_sample_to_np)\n",
    "    .to_iter_dataset()\n",
    "    .batch(batch_size=2)  # batches consecutive elements\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98d9ef",
   "metadata": {},
   "source": [
    "With `IterDataset` we can use Python built-ins, `iter` and `next`, to interact with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b568eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['7bd22aba-afc9-11e6-8293-c4b301cdf627',\n",
       "        'com.mantz_it.rfanalyzer',\n",
       "        array([  101,  2573,  2092,  2007,  2026, 20578, 12881, 11504,  2047,\n",
       "               14409,  2097,  7180,  2005,  4469,  4972,   102])             ,\n",
       "        1469145600.0, 5, 1487],\n",
       "       ['7bd227d9-afc9-11e6-aba1-c4b301cdf627',\n",
       "        'com.mantz_it.rfanalyzer',\n",
       "        array([  101,  2307, 10439,   999,  1996,  2047,  2544,  2085,  2573,\n",
       "                2006,  2026, 11655,  9035, 11924,  2694,  2029,  2003,  2307,\n",
       "                2004,  2009,  1005,  1055,  2157,  2011,  2026, 23308,  9682,\n",
       "                5830,  1012,  1996, 13594,  3444,  2052,  2022,  6179,  1012,\n",
       "                1012,  1012,  2151, 27859,  2006,  2043,  2023,  2097,  2022,\n",
       "                2800,  1029,  2036,  1996,  5724,  2000, 12324,  1037,  2862,\n",
       "                1997,  2338, 27373,  1041,  1012,  1043,  1012,  2013,  1037,\n",
       "                3722,  5144,  5371,  2052,  2022,  6179,  1012,   102])      ,\n",
       "        1476230400.0, 4, 1487]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467c2ae",
   "metadata": {},
   "source": [
    "And that's it! We ended up with a batch with processed date, tokenized review, and filtered rating."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3",
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
