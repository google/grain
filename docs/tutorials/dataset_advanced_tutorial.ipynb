{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ufbgPooUPJr"
   },
   "source": [
    "# Advanced `Dataset` usage\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/grain/blob/main/docs/tutorials/dataset_advanced_tutorial.ipynb)\n",
    "\n",
    "If you decided to use `Dataset` APIs, there's a good chance you want to do one or more processing steps described in this section, especially if working on data ingestion for generative model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFw1tjvkP3wb"
   },
   "outputs": [],
   "source": [
    "# @test {\"output\": \"ignore\"}\n",
    "!pip install grain\n",
    "# @test {\"output\": \"ignore\"}\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4380,
     "status": "ok",
     "timestamp": 1744147018391,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "fwvOt8-cqcQn"
   },
   "outputs": [],
   "source": [
    "import grain\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ur4szH9l5_H"
   },
   "source": [
    "## Checkpointing\n",
    "\n",
    "We provide `Checkpoint{Save|Restore}` to checkpoint the\n",
    "`DatasetIterator`. It is recommended to use it with\n",
    "[Orbax](https://orbax.readthedocs.io/en/latest/index.html), which can checkpoint\n",
    "both, input pipeline and model, and handles the edge cases for distributed\n",
    "training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3250,
     "status": "ok",
     "timestamp": 1744147032891,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "Tf-4Ljd2l5_H",
    "outputId": "367f81d5-4437-4d74-b22a-537039393921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n",
      "1 4\n",
      "2 0\n",
      "3 1\n"
     ]
    }
   ],
   "source": [
    "ds = (\n",
    "    grain.MapDataset.source(tfds.data_source(\"mnist\", split=\"train\"))\n",
    "    .seed(seed=45)\n",
    "    .shuffle()\n",
    "    .to_iter_dataset()\n",
    ")\n",
    "\n",
    "num_steps = 4\n",
    "ds_iter = iter(ds)\n",
    "\n",
    "# Read some elements.\n",
    "for i in range(num_steps):\n",
    "  x = next(ds_iter)\n",
    "  print(i, x[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wb2y5VoTl5_H"
   },
   "outputs": [],
   "source": [
    "# @test {\"output\": \"ignore\"}\n",
    "!pip install orbax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1744147062733,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "PGn-eSYil5_H",
    "outputId": "7835b15f-5607-4f4c-b16a-22164f51365e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/orbax:\n",
      "4\n",
      "\n",
      "/tmp/orbax/4:\n",
      "_CHECKPOINT_METADATA\n",
      "default\n",
      "\n",
      "/tmp/orbax/4/default:\n",
      "process_0-of-1.json\n"
     ]
    }
   ],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "\n",
    "mngr = ocp.CheckpointManager(\"/tmp/orbax\")\n",
    "\n",
    "!rm -rf /tmp/orbax\n",
    "\n",
    "# Save the checkpoint.\n",
    "assert mngr.save(\n",
    "    step=num_steps, args=grain.checkpoint.CheckpointSave(ds_iter), force=True\n",
    ")\n",
    "# Checkpoint saving in Orbax is asynchronous by default, so we'll wait until\n",
    "# finished before examining checkpoint.\n",
    "mngr.wait_until_finished()\n",
    "\n",
    "# @test {\"output\": \"ignore\"}\n",
    "!ls -R /tmp/orbax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1744147066136,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "F012QoCJl5_H",
    "outputId": "82d01250-df6d-4398-deb6-d2c7cb9d301c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"next_index\": 4\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /tmp/orbax/*/*/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1744147068255,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "HURK2viXl5_H",
    "outputId": "1fc99ff2-968d-47f0-f863-78725481f8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 7\n",
      "5 4\n",
      "6 8\n",
      "7 0\n"
     ]
    }
   ],
   "source": [
    "# Read more elements and advance the iterator.\n",
    "for i in range(4, 8):\n",
    "  x = next(ds_iter)\n",
    "  print(i, x[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1744147072103,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "u92Vkn1Hl5_H",
    "outputId": "1a294554-acf1-4398-fa61-d7bb1240ae75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 7\n",
      "5 4\n",
      "6 8\n",
      "7 0\n"
     ]
    }
   ],
   "source": [
    "# Restore iterator from the previously saved checkpoint.\n",
    "mngr.restore(num_steps, args=grain.checkpoint.CheckpointRestore(ds_iter))\n",
    "# Iterator should be set back to start from 4.\n",
    "for i in range(4, 8):\n",
    "  x = next(ds_iter)\n",
    "  print(i, x[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfA_bctscNyV"
   },
   "source": [
    "## Mixing datasets\n",
    "\n",
    "`Dataset` allows mixing multiple data sources with potentially different transformations. There's two different ways of mixing `Dataset`s: `MapDataset.mix` and `IterDataset.mix`. If the mixed `Datasets` are sparse (e.g. one of the mixture components needs to be filtered) use `IterDataset.mix`, otherwise use `MapDataset.mix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1294,
     "status": "ok",
     "timestamp": 1744147084144,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "e8ROZXhtwOx3",
    "outputId": "8d297df8-137d-4d7a-f7fe-50e2f27774fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed dataset length = 6728\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "tfds.core.DatasetInfo.file_format = (\n",
    "    tfds.core.file_adapters.FileFormat.ARRAY_RECORD\n",
    ")\n",
    "# This particular dataset mixes medical images with hand written numbers,\n",
    "# probably not useful but allows to illustrate the API on small datasets.\n",
    "source1 = tfds.data_source(name=\"pneumonia_mnist\", split=\"train\")\n",
    "source2 = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds1 = grain.MapDataset.source(source1).map(lambda features: features[\"image\"])\n",
    "ds2 = grain.MapDataset.source(source2).map(lambda features: features[\"image\"])\n",
    "ds = grain.MapDataset.mix([ds1, ds2], weights=[0.7, 0.3])\n",
    "print(f\"Mixed dataset length = {len(ds)}\")\n",
    "pprint(np.shape(ds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crR2FZ1Gf6-O"
   },
   "source": [
    "If filtering inputs to the mixture, use `IterDataset.mix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1594,
     "status": "ok",
     "timestamp": 1744147093681,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "DTmUbvK4r8T8",
    "outputId": "b1bbf184-5edb-49ce-bcae-01face49d199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "source1 = tfds.data_source(name=\"pneumonia_mnist\", split=\"train\")\n",
    "source2 = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds1 = (\n",
    "    grain.MapDataset.source(source1)\n",
    "    .filter(lambda features: int(features[\"label\"]) == 1)\n",
    "    .to_iter_dataset()\n",
    ")\n",
    "ds2 = (\n",
    "    grain.MapDataset.source(source2)\n",
    "    .filter(lambda features: int(features[\"label\"]) > 4)\n",
    "    .to_iter_dataset()\n",
    ")\n",
    "\n",
    "ds = grain.IterDataset.mix([ds1, ds2], weights=[0.7, 0.3]).map(\n",
    "    lambda features: features[\"image\"]\n",
    ")\n",
    "pprint(np.shape(next(iter(ds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TKInCDc6GUH"
   },
   "source": [
    "### Multi-epoch training\n",
    "\n",
    "Mixed dataset length is determined by a combination of the length of the shortest input dataset and mixing weights. This means that once the shortest component is exhausted the new epoch will begin and the remainder of other datasets is going to be discarded. This can be avoided by repeating inputs to the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1744147102879,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "JqetaYR36GUH",
    "outputId": "525d506c-b8c8-42bb-dc62-2f2aa0fea661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed dataset length = 9223372036854775807\n",
      "Mixed dataset length = 9223372036854775807\n",
      "Mixed dataset length = 9223372036854775807\n"
     ]
    }
   ],
   "source": [
    "source1 = tfds.data_source(name=\"pneumonia_mnist\", split=\"train\")\n",
    "source2 = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds1 = grain.MapDataset.source(source1).repeat()\n",
    "ds2 = grain.MapDataset.source(source2).repeat()\n",
    "\n",
    "ds = grain.MapDataset.mix([ds1, ds2], weights=[1, 2])\n",
    "print(f\"Mixed dataset length = {len(ds1)}\")  # sys.maxsize\n",
    "print(f\"Mixed dataset length = {len(ds2)}\")  # sys.maxsize\n",
    "# Ds1 and ds2 are repeated to fill out the sys.maxsize with respect to weights.\n",
    "print(f\"Mixed dataset length = {len(ds)}\")  # sys.maxsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aulM2cVQlneY"
   },
   "source": [
    "### Shuffling\n",
    "\n",
    "If you need to globally shuffle the mixed data prefer shuffling individual\n",
    "`Dataset`s before mixing. This will ensure that the actual weights of the mixed\n",
    "`Dataset`s are stable and as close as possible to the provided weights.\n",
    "\n",
    "Additionally, make sure to provide different seeds to different mixture\n",
    "components. This way there's no chance of introducing a seed dependency between\n",
    "the components if the random transformations overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1096,
     "status": "ok",
     "timestamp": 1744147114851,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "OTveP3UQE7xv",
    "outputId": "fe861177-11af-4a8c-aa22-b19bbb18e8b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed dataset length = 9223372036854775807\n"
     ]
    }
   ],
   "source": [
    "source1 = tfds.data_source(name=\"pneumonia_mnist\", split=\"train\")\n",
    "source2 = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds1 = grain.MapDataset.source(source1).seed(42).shuffle().repeat()\n",
    "ds2 = grain.MapDataset.source(source2).seed(43).shuffle().repeat()\n",
    "\n",
    "ds = grain.MapDataset.mix([ds1, ds2], weights=[1, 2])\n",
    "print(f\"Mixed dataset length = {len(ds)}\")  # sys.maxsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLsJtcAE8FPu"
   },
   "source": [
    "## Prefetching\n",
    "\n",
    "Grain offers prefetching mechanisms for potential performance improvements.\n",
    "\n",
    "### Multithread prefetching\n",
    "\n",
    "`ThreadPrefetchIterDataset` allows to process the buffer of size\n",
    "`cpu_buffer_size` on the CPU ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Uq4EOb8DAMX6"
   },
   "outputs": [],
   "source": [
    "import grain\n",
    "import jax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "cpu_buffer_size = 3\n",
    "source = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds = grain.MapDataset.source(source).to_iter_dataset()\n",
    "ds.map(lambda x: x)  # Dummy map to illustrate the usage.\n",
    "ds = grain.experimental.ThreadPrefetchIterDataset(ds, prefetch_buffer_size=cpu_buffer_size)\n",
    "ds = ds.map(jax.device_put)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hfinzxFAOcA"
   },
   "source": [
    "`grain.experimental.device_put` allows for processing the buffer of size\n",
    "cpu_buffer_size on the CPU ahead of time and transferring the buffer of size\n",
    "tpu_buffer_size on the device which can be `jax.Device` or\n",
    "`jax.sharding.Sharding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAZz4YMMAPX5"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "cpu_buffer_size = 3\n",
    "tpu_buffer_size = 2\n",
    "source = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds = grain.MapDataset.source(source).to_iter_dataset()\n",
    "ds.map(lambda x: x)  # Dummy map to illustrate the usage.\n",
    "\n",
    "devices = jax.devices()\n",
    "\n",
    "mesh = jax.sharding.Mesh(np.array(devices), axis_names=('data',))\n",
    "p = jax.sharding.PartitionSpec('data')\n",
    "sharding = jax.sharding.NamedSharding(mesh, p)\n",
    "\n",
    "ds = grain.experimental.device_put(\n",
    "        ds=ds,\n",
    "        device=sharding,\n",
    "        cpu_buffer_size=cpu_buffer_size,\n",
    "        device_buffer_size=tpu_buffer_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRbSK9rkAtDC"
   },
   "source": [
    "### Multiprocess Prefetch\n",
    "\n",
    "`MultiprocessPrefetchIterDataset` allows to process the IterDataset in parallel\n",
    "on multiple processes. The `MultiprocessingOptions` allows to specify\n",
    "`num_workers`, `per_worker_buffer_size`, `enable_profiling`.\n",
    "\n",
    "Multiple processes can speed up the pipeline if it's compute bound and\n",
    "bottlenecked on the CPython's GIL. The default value of 0 means no Python\n",
    "multiprocessing, and as a result all data loading and transformation will run in\n",
    "the main Python process.\n",
    "\n",
    "`per_worker_buffer_size`: Size of the buffer for preprocessed elements that each\n",
    "worker maintains. These are elements after all transformations. If your\n",
    "transformations include batching this means a single element is a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G80HqEJDCbZU"
   },
   "outputs": [],
   "source": [
    "import grain\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "source = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds = grain.MapDataset.source(source).to_iter_dataset()\n",
    "\n",
    "prefetch_lazy_iter_ds = ds.mp_prefetch(\n",
    "        grain.MultiprocessingOptions(num_workers=3, per_worker_buffer_size=10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMMpw2LLNDii"
   },
   "source": [
    "### Multiprocess Prefetch Autotune\n",
    "\n",
    "`MultiprocessPrefetchIterDataset` can leverage the autotuning feature to\n",
    "automatically choose the number of workers based on the user provided RAM memory\n",
    "constraint and element size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvEDL_b7M-S1"
   },
   "outputs": [],
   "source": [
    "import grain\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "source = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds = grain.MapDataset.source(source).to_iter_dataset()\n",
    "\n",
    "performance_config = grain.experimental.pick_performance_config(\n",
    "        ds=ds,\n",
    "        ram_budget_mb=1024,\n",
    "        max_workers=None,\n",
    "    )\n",
    "\n",
    "prefetch_lazy_iter_ds = ds.mp_prefetch(\n",
    "        performance_config.multiprocessing_options,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
