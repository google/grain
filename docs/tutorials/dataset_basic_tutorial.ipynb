{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBD-hzAdSv7F"
   },
   "source": [
    "# `Dataset` basics\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/grain/blob/main/docs/tutorials/dataset_basic_tutorial.ipynb)\n",
    "\n",
    "`Dataset` is a low-level API that uses chaining syntax to define data\n",
    "transformation steps. It allows more general types of processing (e.g. dataset\n",
    "mixing) and more control over the execution (e.g. different order of data\n",
    "sharding and shuffling). `Dataset` transformations are composed in a way that\n",
    "allows to preserve random access property past the source and some of the\n",
    "transformations. This, among other things, can be used for debugging by\n",
    "evaluating dataset elements at specific positions without processing the entire\n",
    "dataset.\n",
    "\n",
    "There are 3 main classes comprising the `Dataset` API: `MapDataset`,\n",
    "`IterDataset`, and `DatasetIterator`. Most data pipelines will start with one or\n",
    "more `MapDataset` (often derived from a `RandomAccessDataSource`) and switch to\n",
    "`IterDataset` late or not at all. The following sections will provide more\n",
    "details about each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvnXLPI_2dNJ"
   },
   "source": [
    "## Install and import Grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHOibn5Q2GRt"
   },
   "outputs": [],
   "source": [
    "# @test {\"output\": \"ignore\"}\n",
    "!pip install grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCZXw2YhhPyu"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import grain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPv3wrQd3pZS"
   },
   "source": [
    "## `MapDataset`\n",
    "\n",
    "`MapDataset` defines a dataset that supports efficient random access. Think of\n",
    "it as an (infinite) `Sequence` that computes values lazily. It will either be\n",
    "the starting point of the input pipeline or in the middle of the pipeline\n",
    "following another `MapDataset`. Grain provides many basic transformations for\n",
    "users to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1734118721729,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "3z3Em5jC2iVz",
    "outputId": "c63e44a8-7a03-4d01-c210-b292ad6c5bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 6, 11])\n",
      "[array([ 6, 11]),\n",
      " array([ 2, 10]),\n",
      " array([3, 5]),\n",
      " array([1, 4]),\n",
      " array([8, 9]),\n",
      " array([7])]\n"
     ]
    }
   ],
   "source": [
    "dataset = (\n",
    "    # You can also use a shortcut grain.MapDataset.range for\n",
    "    # range-like input.\n",
    "    grain.MapDataset.source([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "    .shuffle(seed=10)  # Shuffles globally.\n",
    "    .map(lambda x: x + 1)  # Maps each element.\n",
    "    .batch(batch_size=2)  # Batches consecutive elements.\n",
    ")\n",
    "\n",
    "pprint(dataset[0])\n",
    "pprint(list(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aii_JDBw5SEI"
   },
   "source": [
    "The requirement for `MapDataset`'s source is a `grain.RandomAccessDataSource`\n",
    "interface: i.e. `__getitem__` and `__len__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "592ut9AgiDCz"
   },
   "outputs": [],
   "source": [
    "# Note: Inheriting `grain.RandomAccessDataSource` is optional but recommended.\n",
    "class MySource(grain.sources.RandomAccessDataSource):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._data = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self._data[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1734118755899,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "m8Cyn7gt6FYy",
    "outputId": "ff59f222-042c-48fb-d426-c2b3da9a2017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([6, 7])\n",
      "[array([6, 7]), array([2, 8]), array([3, 5]), array([1, 4])]\n"
     ]
    }
   ],
   "source": [
    "source = MySource()\n",
    "\n",
    "dataset = (\n",
    "    grain.MapDataset.source(source)\n",
    "    .shuffle(seed=10)  # Shuffles globally.\n",
    "    .map(lambda x: x + 1)  # Maps each element.\n",
    "    .batch(batch_size=2)  # Batches consecutive elements.\n",
    ")\n",
    "\n",
    "pprint(dataset[0])\n",
    "pprint(list(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKv2kWjB6XPd"
   },
   "source": [
    "Access by index will never raise an `IndexError` and can treat indices that are\n",
    "equal or larger than the length as a different epoch (e.g. shuffle differently,\n",
    "use different random numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734118760614,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "GSW1cJe06NEO",
    "outputId": "ffe2b9e8-069c-45f1-ac93-c391604b5b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([7, 3])\n"
     ]
    }
   ],
   "source": [
    "# Prints the 3rd element of the second epoch.\n",
    "pprint(dataset[len(dataset) + 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azfAr8F37njE"
   },
   "source": [
    "Note that `dataset[idx] == dataset[len(dataset) + idx]` iff there's no random\n",
    "transfomations. Since `dataset` has global shuffle, different epochs are\n",
    "shuffled differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734118766095,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "_o3wxb8k7XDY",
    "outputId": "f4c2a263-0084-45d3-dd0f-51f58c96bead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([False, False])\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[len(dataset) + 2] == dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2kLX0fa8GfV"
   },
   "source": [
    "You can use `filter` to remove elements not needed but it will return `None` to\n",
    "indicate that there is no element at the given index.\n",
    "\n",
    "Returning `None` for the majority of positions can negatively impact performance\n",
    "of the pipeline. For example, if your pipeline filters 90% of the data it might\n",
    "be better to store a filtered version of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1734118794030,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "ai4zcltV7sSN",
    "outputId": "c8818ad2-c7d7-414f-8359-0bd2e679b9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Length of this dataset: 4'\n",
      "[None, array([2, 8]), array([3, 5]), None]\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = dataset.filter(lambda e: (e[0] + e[1]) % 2 == 0)\n",
    "\n",
    "pprint(f\"Length of this dataset: {len(filtered_dataset)}\")\n",
    "pprint([filtered_dataset[i] for i in range(len(filtered_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJLK_BQj9GuG"
   },
   "source": [
    "`MapDataset` also supports slicing using the same syntax as Python lists. This\n",
    "returns a `MapDataset` representing the sliced section. Slicing is the easiest\n",
    "way to \"shard\" data during distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1734118798792,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "-fuS_OGS8x5Z",
    "outputId": "7ce27bed-0adb-43a4-8abb-8d48937f1e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharded dataset length = 2\n",
      "array([6, 7])\n",
      "array([3, 5])\n"
     ]
    }
   ],
   "source": [
    "shard_index = 0\n",
    "shard_count = 2\n",
    "\n",
    "sharded_dataset = dataset[shard_index::shard_count]\n",
    "print(f\"Sharded dataset length = {len(sharded_dataset)}\")\n",
    "pprint(sharded_dataset[0])\n",
    "pprint(sharded_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvycxocM-Fpk"
   },
   "source": [
    "For the actual running training with the dataset, we should convert `MapDataset`\n",
    "into `IterDataset` to leverage parallel prefetching to hide the latency of each\n",
    "element's IO using Python threads.\n",
    "\n",
    "This brings us to the next section of the tutorial: `IterDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1734118801247,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FnWPIpce9aAJ",
    "outputId": "dba2951e-a965-4dd3-816c-dcbbea6352f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([6, 7])\n",
      "array([3, 5])\n"
     ]
    }
   ],
   "source": [
    "iter_dataset = sharded_dataset.to_iter_dataset(\n",
    "    grain.ReadOptions(num_threads=16, prefetch_buffer_size=500)\n",
    ")\n",
    "\n",
    "for element in iter_dataset:\n",
    "  pprint(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-Brm4Mh_Bo1"
   },
   "source": [
    "## IterDataset\n",
    "\n",
    "Most data pipelines will start with one or more `MapDataset` (often derived from\n",
    "a `RandomAccessDataSource`) and switch to `IterDataset` late or not at all.\n",
    "`IterDataset` does not support efficient random access and only supports\n",
    "iterating over it. It's an `Iterable`.\n",
    "\n",
    "Any `MapDataset` can be turned into a `IterDataset` by calling\n",
    "`to_iter_dataset`. When possible this should happen late in the pipeline since\n",
    "it will restrict the transformations that can come after it (e.g. global shuffle\n",
    "must come before). This conversion by default skips `None` elements.\n",
    "\n",
    "Some transformations have implementations for both, `MapDataset` and\n",
    "`IterDataset`, e.g. `filter`, `map`, `random_map`, `batch`. They produce the\n",
    "same result with one caveat: `MapDataset.batch` cannot follow\n",
    "`MapDataset.filter` - you will need to convert to `IterDataset` before applying\n",
    "`batch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1746224728172,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "AKJhIi0VGSCR",
    "outputId": "c4803f59-026b-49a8-c45b-cbe9f856037d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 2]), array([4, 6]), array([8])]\n"
     ]
    }
   ],
   "source": [
    "ds = (\n",
    "    grain.MapDataset.range(10)\n",
    "    .filter(lambda x: x % 2 == 0)\n",
    "    .to_iter_dataset()\n",
    "    .batch(2)  # Calling `batch` before `to_iter_dataset` will raise an error.\n",
    ")\n",
    "pprint(list(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDO1u2tQ_zPz"
   },
   "source": [
    "`DatasetIterator` is a stateful iterator of `IterDataset`. The state of the\n",
    "iterator can be cheaply saved and restored. This is intended for checkpointing\n",
    "the input pipeline together with the trained model. The returned state will not\n",
    "contain data that flows through the pipeline.\n",
    "\n",
    "Essentially, `DatasetIterator` only checkpoints index information for it to\n",
    "recover (assuming the underlying content of files will not change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1734118805719,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "DRgatGFX_nxL",
    "outputId": "5ec46759-41a9-4211-c856-3f46c2ee2a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dataset_iter = iter(dataset)\n",
    "pprint(isinstance(dataset_iter, grain.DatasetIterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1734118814192,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "dOCiJfSJ_vi4",
    "outputId": "6e010a76-11e3-4aad-ef16-93c00aa6ae27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([6, 7])\n",
      "array([2, 8])\n",
      "array([2, 8])\n"
     ]
    }
   ],
   "source": [
    "pprint(next(dataset_iter))\n",
    "\n",
    "checkpoint = dataset_iter.get_state()\n",
    "\n",
    "pprint(next(dataset_iter))\n",
    "\n",
    "# Recover the iterator to the state after the first produced element.\n",
    "dataset_iter.set_state(checkpoint)\n",
    "\n",
    "pprint(next(dataset_iter))  # This should generate the same element as above"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
